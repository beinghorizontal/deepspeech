{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepSpeech_train_a_model,_alex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beinghorizontal/deepspeech/blob/main/DeepSpeech_train_a_model%2C_alex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQLBTNM6EmH7"
      },
      "source": [
        "# Alex speech-to-text model; Training\n",
        "## Using DeepSpeech and my own data\n",
        "\n",
        "*Trying to use my own custom wav files and train the model using the article written by an Indian guy with his own custom indian accent dataset*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDwSuUvbEyp-"
      },
      "source": [
        "##Training Your Own Model\n",
        "*Taken from the [DeepSpeech docs - Training Your Own Model](https://deepspeech.readthedocs.io/en/v0.7.4/TRAINING.html#training-your-own-model)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prevent random disconnects\n",
        "\n",
        "This cell runs JS code to automatic reconnect to runtime."
      ],
      "metadata": {
        "id": "U0MEowcx9aAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\"); \n",
        "     btn.click() \n",
        "     }\n",
        "   \n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\"); \n",
        "     btn.click() \n",
        "     }\n",
        "  }\n",
        "  \n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "Utl4dDIB9a9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gpu type"
      ],
      "metadata": {
        "id": "RmKsM-fAGRNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "qq2MW8VzGcX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWLqs-SmnIBN"
      },
      "source": [
        "import sys\n",
        "! sudo apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf1soWCmFGwX"
      },
      "source": [
        "### Get the training code and checkpoints\n",
        "\n",
        "Then clone the DeepSpeech repository and run `git lfs pull`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZHbbBawxIkf"
      },
      "source": [
        "! git clone --branch v0.9.3 https://github.com/mozilla/DeepSpeech\n",
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-checkpoint.tar.gz\n",
        "#%mkdir /content/model_checkpoints\n",
        "!tar -xvf /content/deepspeech-0.9.3-checkpoint.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DeepSpeech\n",
        "! git lfs pull"
      ],
      "metadata": {
        "id": "AsNI1W0shC6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DeepSpeech\n",
        "!pip3 install --upgrade pip==20.2.2 wheel==0.34.2 setuptools==49.6.0\n",
        "!pip3 install --upgrade -e .\n"
      ],
      "metadata": {
        "id": "_HmRe5D0hlk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y!pip3 uninstall tensorflow\n",
        "!pip3 install 'tensorflow-gpu==1.15.4'"
      ],
      "metadata": {
        "id": "tCSTVTqBiIxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/git-lfs/git-lfs/releases/download/v2.11.0/git-lfs-linux-amd64-v2.11.0.tar.gz\n",
        "!tar xvf /content/git-lfs-linux-amd64-v2.11.0.tar.gz -C /content\n",
        "!sudo bash /content/install.sh\n",
        "%cd /content/DeepSpeech\n",
        "!git-lfs pull"
      ],
      "metadata": {
        "id": "SarSdYefOune"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/ds_ctcdecoder-0.9.3-cp37-cp37m-manylinux1_x86_64.whl\n",
        "!pip install /content/DeepSpeech/ds_ctcdecoder-0.9.3-cp37-cp37m-manylinux1_x86_64.whl"
      ],
      "metadata": {
        "id": "YOEvwKAhPNsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9tAsBsfFmoY"
      },
      "source": [
        "Once this command completes successfully, the environment will be ready to be activated."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get pre trained checkpoints from Drive \n",
        "\n"
      ],
      "metadata": {
        "id": "MHbVZxR84PoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import from Drive\n",
        "\n",
        "Mode = \"mycheckpoints\" #@param [\"speech\", \"mycheckpoints\", \"data_dst\", \"models\"]\n",
        "Archive_name = \"mycheckpoints.zip\" #@param [\"Scripts_t2.zip\", \"mycheckpoints.zip\"]\n",
        "\n",
        "#Mount Google Drive as folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def zip_and_copy(path, mode):\n",
        "  unzip_cmd=\" -q \"+Archive_name\n",
        "  \n",
        "  %cd $path\n",
        "  copy_cmd = \"/content/drive/My\\ Drive/\"+Archive_name+\" \"+path\n",
        "  !cp $copy_cmd\n",
        "  !unzip $unzip_cmd    \n",
        "  !rm $Archive_name\n",
        "\n",
        "zip_and_copy(\"/content\", \"nl\")\n",
        "  \n",
        "print(\"Done!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "E94ECmbv4Ox3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get raw data from Drive \n"
      ],
      "metadata": {
        "id": "75mjiC8Rk_Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import from Drive\n",
        "\n",
        "Mode = \"speech\" #@param [\"speech\", \"data_src\", \"data_dst\", \"data_src aligned\", \"data_dst aligned\", \"models\"]\n",
        "Archive_name = \"Scripts_t2.zip\" #@param {type:\"string\"}\n",
        "\n",
        "#Mount Google Drive as folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def zip_and_copy(path, mode):\n",
        "  unzip_cmd=\" -q \"+Archive_name\n",
        "  \n",
        "  %cd $path\n",
        "  copy_cmd = \"/content/drive/My\\ Drive/\"+Archive_name+\" \"+path\n",
        "  !cp $copy_cmd\n",
        "  !unzip $unzip_cmd    \n",
        "  !rm $Archive_name\n",
        "\n",
        "zip_and_copy(\"/content\", \"nl\")\n",
        "  \n",
        "print(\"Done!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "T5pPnuI-lEcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create csv in deepspeech readable format\n",
        "## install num2words"
      ],
      "metadata": {
        "id": "FlyXalVcHCpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install num2words"
      ],
      "metadata": {
        "id": "lBUpwVA6HVAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# import string\n",
        "# import csv\n",
        "# import argparse\n",
        "import librosa                  # pip install librosa==0.7.2\n",
        "import num2words                # pip install num2words\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def replace_func(text):\n",
        "    \"\"\"Remove extra characters from the transcript which are not in DeepSpeech's alphabet.txt\n",
        "    \"\"\"\n",
        "\n",
        "    for ch in ['\\\\', '`', '‘', '’', '*', '_', ',', '\"', '{', '}', '[', ']', '(', ')', '>', '#', '+', '-', '.', '!', '$', ':', ';', '|', '~', '@', '*', '<', '?', '/']:\n",
        "        if ch in text:\n",
        "            text = text.replace(ch, \"\")\n",
        "        elif ch == '&':\n",
        "            text = text.replace(ch, \"and\")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_audio_info(file_name):\n",
        "    \"\"\"Return specified audio file duration and sample rate\n",
        "    \"\"\"\n",
        "\n",
        "    return librosa.get_duration(filename=file_name), librosa.get_samplerate(file_name)\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description=\"Pre Process File\")\n",
        "# parser.add_argument('--wav', required=True,\n",
        "#                     help='WAV Folder')\n",
        "# parser.add_argument('--meta', required=True,\n",
        "#                     help='Path to file with metadata')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# File containing audio file name and transcript\n",
        "wav_file =  '/content/wave_data/Recording_'\n",
        "trans_file = '/content/trans_data/Recording_'\n",
        "# in_file = open(os.path.join(args.meta, 'txt'), 'r')\n",
        "\n",
        "# Create target CSV file to write metadata info as per DeepSpeech requirements\n",
        "# Define a writer object to write rows to file\n",
        "\n",
        "# out_file = open(os.path.join(os.getcwd(), 'output.csv'), 'a', newline='')\n",
        "# csv_writer = csv.writer(out_file)\n",
        "\n",
        "# All CSV files must contain the following as the first line. Only run once\n",
        "# df = pd.DataFrame({'wav_filename':[], 'wav_filesize':[], 'transcript':[]})\n",
        "# df.to_csv('main.csv',index=False)\n",
        "# csv_writer.writerow(['wav_filename', 'wav_filesize', 'transcript'])\n",
        "\n",
        "# Keep track of total audio files and files not added to CSV due to them being too long or invalid sample rate\n",
        "total_count = 0\n",
        "row_count = 0\n",
        "wav_l =os.listdir('/content/wave_data')\n",
        "trans_l = os.listdir('/content/trans_data')\n",
        "\n",
        "try:\n",
        "    df_list = []\n",
        "\n",
        "    for line in range(len(wav_l)):\n",
        "        # line = 6\n",
        "        total_count += 1\n",
        "\n",
        "        try:\n",
        "            # fname, ftext, _ = line.split(\"\\\"\")\n",
        "\n",
        "            # Separate file name and transcript from metadata file. Preprocess transcript and get audio info too\n",
        "            # convert all numbers to text using num2words\n",
        "            # fname = fname.strip()[1:].strip() + '.wav'\n",
        "            with open(trans_file+str(line)+'.txt', 'r') as w:\n",
        "                ftext = w.read()\n",
        "            # print(ftext)\n",
        "\n",
        "\n",
        "            ftext = ftext.strip().lower()\n",
        "            ftext = replace_func(ftext).replace(\"  \", \" \").strip()\n",
        "            ftext = replace_func(ftext).replace(\"\\n\", \". \").strip()\n",
        "            ftext = replace_func(ftext).replace(\"&\",\"and\")\n",
        "            ftext = replace_func(ftext).replace(\".\",\"\")\n",
        "            ftext = replace_func(ftext).replace(\" . \",\"\")\n",
        "            ftext = replace_func(ftext).replace(\". . \",\".\")\n",
        "            ftext = replace_func(ftext).replace(\" .\",\"\")\n",
        "\n",
        "            ftext = re.sub(r\"(\\d+)\", lambda x: num2words.num2words(int(x.group(0))), ftext)\n",
        "\n",
        "            wav_filename = wav_file+str(line)+'.wav'\n",
        "            fdur, fsr = get_audio_info(wav_filename)\n",
        "\n",
        "            # Don't add files which don't fit into model specifications\n",
        "            # Either not 48kHz or longer than 30 secs\n",
        "            if fsr != 48000:\n",
        "                print(\"Different SR:\", wav_filename)\n",
        "                continue\n",
        "            if fdur > 60:\n",
        "                print(\"Too Long:\", wav_filename)\n",
        "                continue\n",
        "            if ftext == '':\n",
        "                print(\"No Transcript found\")\n",
        "                continue\n",
        "\n",
        "            # Write each row to CSV with size info\n",
        "            fsize = os.path.getsize(wav_filename)\n",
        "            df_small = pd.DataFrame([wav_filename, fsize, ftext]).transpose()\n",
        "            df_list.append(df_small)\n",
        "\n",
        "            print(wav_filename, fsize, ftext)\n",
        "            # csv_writer.writerow([wav_filename, fsize, ftext])\n",
        "            row_count += 1\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "\n",
        "except Exception as e:\n",
        "    print(str(e))\n",
        "\n",
        "print(\"Added Rows:\", row_count)\n",
        "print(\"Total Rows:\", total_count, \"\\n\")\n",
        "df_final = pd.concat(df_list)\n",
        "col_list = ['wav_filename', 'wav_filesize', 'transcript']\n",
        "df_final.columns = col_list\n",
        "df_final.to_csv('output.csv',index=False)\n",
        "# in_file.close()\n",
        "# out_file.close()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "LbZQcJMrHIxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split output csv to train, test, dev files "
      ],
      "metadata": {
        "id": "UN7yFw3eIZZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "file = \"/content/output.csv\"\n",
        "\n",
        "df1 = pd.read_csv(file)\n",
        "df1['split'] = np.random.randn(df1.shape[0], 1)\n",
        "# Split ratio for training set\n",
        "msk = np.random.rand(len(df1)) <= 0.6\n",
        "train = df1[msk]\n",
        "inter = df1[~msk]\n",
        "train.to_csv('/content/train.csv', index=False)\n",
        "inter.to_csv('/content/intermediate.csv', index=False)\n",
        "\n",
        "df2 = pd.read_csv('/content/intermediate.csv')\n",
        "df2['split'] = np.random.randn(df2.shape[0], 1)\n",
        "# Split ratio for dev and test\n",
        "msk = np.random.rand(len(df2)) <= 0.5\n",
        "dev = df2[msk]\n",
        "test = df2[~msk]\n",
        "dev.to_csv('/content/dev.csv', index=False)\n",
        "test.to_csv('/content/test.csv', index=False)\n"
      ],
      "metadata": {
        "id": "7m_a-FagIhlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azh5ZBEURTDH"
      },
      "source": [
        "Providing a filter alphabet is optional. It will exclude all samples whose transcripts contain characters not in the specified alphabet. Running the importer with -h will show you some additional options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPM0qQZg5011"
      },
      "source": [
        "#%cd /content/DeepSpeech\n",
        "\n",
        "#!python3 /content/DeepSpeech/training/deepspeech_training/util/check_characters.py -alpha -unicode -csv /content/train.csv\n",
        "#!python3 /content/DeepSpeech/training/deepspeech_training/util/check_characters.py -csv /content/dev.csv -alpha\n",
        "#!python3 /content/DeepSpeech/training/deepspeech_training/util/check_characters.py -csv /content/test.csv -alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove folders"
      ],
      "metadata": {
        "id": "jO4QXjKRQeaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf \"/content/mycheckpoints\""
      ],
      "metadata": {
        "id": "nM_WFnAtQkdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Em9kDZ1sZURs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/models"
      ],
      "metadata": {
        "id": "mfpqy2KN2qDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DeepSpeech/\n",
        "! python3 DeepSpeech.py \\\n",
        "  --train_files /content/train.csv \\\n",
        "  --dev_files /content/dev.csv \\\n",
        "  --test_files /content/test.csv \\\n",
        "  --n_hidden 2048 \\\n",
        "  --checkpoint_dir /content/mycheckpoints \\\n",
        "  --learning_rate 0.0001 \\\n",
        "  --train_cudnn True \\\n",
        "  --train_batch_size 1 \\\n",
        "  --test_batch_size 1 \\\n",
        "  --epochs 300 \\\n",
        "  --max_to_keep 1 \\\n",
        "  --audio_sample_rate 48000 \\\n",
        "  --export_file_name 'ft_model' \\\n",
        "  --export_dir /content/models/"
      ],
      "metadata": {
        "id": "Xs1Y6kwYZW-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export new checkpoints to drive\n"
      ],
      "metadata": {
        "id": "qM9Fw4iRkNZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Removing old data in mycheckpoints which is not useful before export to drive\")\n",
        "import os\n",
        "\n",
        "root = '/content/mycheckpoints'\n",
        "\n",
        "best_checkpath = root+'/best_dev_checkpoint'\n",
        "checkpath = root +'/checkpoint'\n",
        "\n",
        "root_files = os.listdir(root)\n",
        "\n",
        "with open(best_checkpath, 'r')as n:\n",
        "    bestnames_file = n.read().splitlines()\n",
        "\n",
        "best_dev_files = []\n",
        "match_text = bestnames_file[0].split('best_dev-')[1]\n",
        "match_text = match_text.replace('\"','')\n",
        "list2 = ['best_dev_checkpoint','checkpoint','flags.txt']\n",
        "not_matching = [s for s in root_files if not match_text in s]\n",
        "\n",
        "#remove files in checkpoint i.e. train data\n",
        "with open(checkpath, 'r')as m:\n",
        "    names_file = m.read().splitlines()\n",
        "\n",
        "best_dev_files = []\n",
        "match_text2 = names_file[0].split('train-')[1]\n",
        "match_text3 = match_text2.replace('\"','')\n",
        "\n",
        "not_matching2 = [t for t in not_matching if not match_text3 in t]\n",
        "\n",
        "new_list = list(set(not_matching2).difference(list2))\n",
        "\n",
        "for remove_file in new_list:\n",
        "    print(root+'/'+remove_file)\n",
        "    os.remove(root+'/'+remove_file)\n",
        "\n",
        "print(\"old data removed, now google drive transfer initiated\")\n",
        "\n",
        "from google.colab import drive\n",
        "Archive_name = \"mycheckpoints.zip\"\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "def zip_and_copy(path, mode):\n",
        "  zip_cmd=\"-r -q \"+Archive_name+\" \"\n",
        "  %cd $path\n",
        "  zip_cmd+=mode\n",
        "  !zip $zip_cmd\n",
        "  copy_cmd = \" \"+Archive_name+\"  /content/drive/My\\ Drive/\"\n",
        "  !cp $copy_cmd\n",
        "  !rm $Archive_name\n",
        "\n",
        "\n",
        "zip_and_copy(\"/content\", \"mycheckpoints\")\n"
      ],
      "metadata": {
        "id": "6-4u_VhqftxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the model for inference and export to drive"
      ],
      "metadata": {
        "id": "qFk3hMOJkBd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DeepSpeech/\n",
        "!python3 util/taskcluster.py --source tensorflow --artifact convert_graphdef_memmapped_format --branch r1.15 --target .\n",
        "!./convert_graphdef_memmapped_format --in_graph=/content/models/ft_model.pb --out_graph=/content/models/ft_model.pbmm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/models\n",
        "!cp ft_model.pbmm /content/drive/MyDrive\n"
      ],
      "metadata": {
        "id": "Hltqz0Bt1iw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It ends here No need to explore further unless you want to make scorer (lang model)\n",
        "Might useful in future though\n"
      ],
      "metadata": {
        "id": "_BkLCViXAla7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRVW0KjZRdyu"
      },
      "source": [
        "###Create a Language Model / Scorer\n",
        "*Taken from [DeepSpeech documentation - External scorer scripts](https://deepspeech.readthedocs.io/en/v0.7.4/Scorer.html)*\n",
        "\n",
        "The LibriSpeech LM training text used by our scorer is around 4GB uncompressed, which should give an idea of the size of a corpus needed for a reasonable language model for general speech recognition. For more constrained use cases with smaller vocabularies, you don’t need as much data, but you should still try to gather as much as you can.\n",
        "\n",
        "*We're using data from the [Europarl corpus](https://www.statmt.org/europarl/) for our Dutch scorer. This was the largest Dutch language dataset already formatted that I could find.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFOvpj71RXL-"
      },
      "source": [
        "%cd /content/DeepSpeech/data/lm\n",
        "! wget https://www.statmt.org/europarl/v7/nl-en.tgz\n",
        "! tar -xzvf nl-en.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZwFjv8RTmYb"
      },
      "source": [
        "Build [KenLM](https://github.com/kpu/kenlm) first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0dprd_bTKSv"
      },
      "source": [
        "%cd /content\n",
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "%mkdir -p /content/kenlm/build\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCJIyskCjbYh"
      },
      "source": [
        "%cd /content/kenlm/build\n",
        "! cmake .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WryUTU31irey"
      },
      "source": [
        "! make -j 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mc5Ic2RMvlT"
      },
      "source": [
        "Then use the `generate_lm.py` script to generate `lm.binary` and `vocab-500000.txt`.\n",
        "\n",
        "As input you can use a plain text (e.g. file.txt) or gzipped (e.g. file.txt.gz) text file with one sentence in each line.\n",
        "\n",
        "Pass the KenLM build directory to the script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmtsPbbzSsxL"
      },
      "source": [
        "%cd /content/DeepSpeech/data/lm\n",
        "! python3 generate_lm.py --input_txt europarl-v7.nl-en.nl --output_dir . \\\n",
        "  --top_k 500000 --kenlm_bins /content/kenlm/build/bin/ \\\n",
        "  --arpa_order 5 --max_arpa_memory \"85%\" --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 --binary_q_bits 8 --binary_type trie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nbp0PQUL0mB"
      },
      "source": [
        "Afterwards you can use `generate_package.py` to generate the scorer package using the `lm.binary` and `vocab-500000.txt` files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1d0Uldnl-7n"
      },
      "source": [
        "%cd /content/DeepSpeech/data/lm/\n",
        "! python3 ./generate_package.py --alphabet /content/nl/alphabet.txt --lm lm.binary --vocab vocab-500000.txt \\\n",
        "  --package kenlm.scorer --default_alpha 0.931289039105002 --default_beta 1.1834137581510284"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVc4GQ28sSjg"
      },
      "source": [
        "Note that we have a `lm_optimizer.py` script which can be used to find good default values for alpha and beta. To use it, you must first generate a package with any value set for default alpha and beta flags. For this step, it doesn’t matter what values you use, as they’ll be overridden by `lm_optimizer.py`. Then, use `lm_optimizer.py` with this scorer file to find good alpha and beta values. Finally, use `generate_package.py` again, this time with the new values.\n",
        "\n",
        "*// NOTE: I coudln't get `lm_optimizer.py` to run, but the scorer was good enough to move on to the next step.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQrQcdpxsSQJ"
      },
      "source": [
        "%cd /content/DeepSpeech/data/lm/\n",
        "! python3 ../../lm_optimizer.py \\\n",
        "  --test_files /content/nl/clips/validated.csv \\\n",
        "  --checkpoint_dir /content/checkpoint-lm \\\n",
        "  --load_evaluate init \\\n",
        "  --scorer kenlm.scorer \\\n",
        "  --alphabet_config_path /content/nl/alphabet.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL61boRsSIT8"
      },
      "source": [
        "###Training a Model\n",
        "*Taken from [DeepSpeech documentation - Training a Model](https://deepspeech.readthedocs.io/en/v0.7.4/TRAINING.html#training-a-model)*\n",
        "\n",
        "The central (Python) script is DeepSpeech.py in the project’s root directory. For its list of command line options, you can call:\n",
        "\n",
        "```python3 DeepSpeech.py --helpfull```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNeCzckKiGRC"
      },
      "source": [
        "%cd /content/DeepSpeech/\n",
        "! python3 DeepSpeech.py \\\n",
        "  --train_files /content/nl/cv-corpus-5-2020-06-22/nl/clips/train.csv \\\n",
        "  --dev_files /content/nl/cv-corpus-5-2020-06-22/nl/clips/dev.csv \\\n",
        "  --test_files /content/nl/cv-corpus-5-2020-06-22/nl/clips/test.csv \\\n",
        "  --train_batch_size 1 \\\n",
        "  --test_batch_size 1 \\\n",
        "  --n_hidden 100 \\\n",
        "  --epochs 100 \\\n",
        "  --checkpoint_dir /content/model_checkpoints/ \\\n",
        "  --export_dir /content/models/ \\\n",
        "  --export_file_name 'ftmodel.pb' \\\n",
        "  --alphabet_config_path ../nl/alphabet.txt \\\n",
        "  --scorer /data/lm/kenlm.scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMYmxX6NyCBj"
      },
      "source": [
        "####Making a mmap-able model for inference\n",
        "\n",
        "The `output_graph.pb` model file generated in the above step will be loaded in memory to be dealt with when running inference. This will result in extra loading time and memory consumption. One way to avoid this is to directly read data from the disk.\n",
        "\n",
        "TensorFlow has tooling to achieve this: it requires building the target `//tensorflow/contrib/util:convert_graphdef_memmapped_format` (binaries are produced by our TaskCluster for some systems including Linux/amd64 and macOS/amd64), use `util/taskcluster.py` tool to download:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHYFVgOJw8g6"
      },
      "source": [
        "%cd /content/DeepSpeech/\n",
        "! python3 util/taskcluster.py --source tensorflow --artifact convert_graphdef_memmapped_format --branch r1.15 --target ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZcI_YJShkW"
      },
      "source": [
        "Producing a mmap-able model is as simple as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7orzRD80rIb"
      },
      "source": [
        "! ./convert_graphdef_memmapped_format --in_graph=/content/model/output_graph.pb --out_graph=/content/model/output_graph.pbmm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qABHNHT3Sj5H"
      },
      "source": [
        "Upon sucessfull run, it should report about conversion of a non-zero number of nodes. If it reports converting `0` nodes, something is wrong: make sure your model is a frozen one, and that you have not applied any incompatible changes (this includes `quantize_weights`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTuTEyEg18r_"
      },
      "source": [
        "## Using a Trained Model\n",
        "*Taken from [DeepSpeech documentation - Using a Pre-trained Model](https://deepspeech.readthedocs.io/en/v0.7.4/USING.html#usage-docs)*\n",
        "\n",
        "*Now that we've trained a model, let's use it!*\n",
        "\n",
        "## Installing DeepSpeech Python bindings\n",
        "\n",
        "Once your environment has been set-up and loaded, you can use `pip3` to manage packages locally. On a fresh setup of the `virtualenv`, you will have to install the DeepSpeech wheel. You can check if `deepspeech` is already installed with `pip3 list`.\n",
        "\n",
        "To perform the installation, just use `pip3` as such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOvuMNwg1pYo"
      },
      "source": [
        "! pip3 install deepspeech-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKgI5YkNOt8H"
      },
      "source": [
        "*Using the model and scorer we created on one of the Common Voice test files. \"ik ben tegen de doodstraf\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xf95vZC0-Qy"
      },
      "source": [
        "! deepspeech --model /content/model/output_graph.pbmm --audio /content/nl/cv-corpus-5-2020-06-22/nl/clips/common_voice_nl_19505086.wav --scorer /content/DeepSpeech/data/lm/kenlm.scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSOPSYhSO63j"
      },
      "source": [
        "*Let's try the model on a new audio file we haven't used yet! Here's a YouTube clip saying 'Stroopwafel'.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB3speyN5QJL"
      },
      "source": [
        "%cd /content\n",
        "! pip install youtube-dl\n",
        "yt=\"https://www.youtube.com/watch?v=oXcvvuO1C7c\"\n",
        "! youtube-dl --extract-audio --audio-format wav {yt}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_x4Nj8iBOxX"
      },
      "source": [
        "from IPython.display import Audio\n",
        "Audio(\"/content/How to pronounce the word stroopwafel - pronunciation_prononciation_pronunciación-oXcvvuO1C7c.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJi6PdDI5iGE"
      },
      "source": [
        "! deepspeech --model /content/model/output_graph.pbmm --audio /content/How\\ to\\ pronounce\\ the\\ word\\ stroopwafel\\ -\\ pronunciation_prononciation_pronunciación-oXcvvuO1C7c.wav --scorer /content/DeepSpeech/data/lm/kenlm.scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVlKW6c6PMnU"
      },
      "source": [
        "*Not the most accurate model, but this is an example of what you can build using DeepSpeech, Common Voice, the Europarl corpus, and the free GPUs and space available on Google Colab.*"
      ]
    }
  ]
}